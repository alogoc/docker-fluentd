# ianblenke/fluentd

This is a highly configurable [fluentd][2] docker image bundled with ElasticSearch, S3, and Cloudwatch plugins.

Please take a second and read through the run.sh script, as an attempt has been made to document it and make it readable.

This container uses supervisord to spawn fluentd and confd simultaneously.

Confd requires an `ETCD_IP` (host) or `ETCD_ADDR` (host:port) to be defined. If neither are found, supervisord will spawn a local etcd for confd to use.

The confd config allows the embedding of confd programmagic expressions like `{{ getenv "VARIABLE" }}`, as well as multi-host service discovery and self-healing fluentd containers communicating with elasticsearch servers.

There is a default configuration dynamically generated by the run.sh script.

Rather than store a config somewhere else publically and curl it down insecurely, this docker image will process a `FLUENTD_CONFIG_ONELINE` formatted string into a config file.

For example, if we start with the default dynamically generated config from the run.sh script:

    <source>
      type monitor_agent
      bind 0.0.0.0
      port 24220
    </source>
    <source>
      type tcp
      port 5170
      format json
      source_host_key client_host
      tag system
    </source>
    <source>
      type tcp
      port 5171
      format nginx
      source_host_key client_host
      tag nginx
    </source>
    <match system>
      type record_reformer
      renew_record false
      enable_ruby false
      remove_keys __CURSOR,__REALTIME_TIMESTAMP,__MONOTONIC_TIMESTAMP,_BOOT_ID,_UID,_GID,_CAP_EFFECTIVE,_SYSTEMD_SLICE,SYSLOG_IDENTIFIER,_SYSTEMD_CGROUP,_CMDLINE,_COMM
      tag system.clean
    </match>
    <match **>
      type elasticsearch
      log_level debug
      include_tag_key true
      hosts 172.17.42.1
      port 9200
      logstash_format true
      reload_on_failure true
      reload_connections true
      buffer_type file
      buffer_path /data/fluentd/buffer/
      flush_interval 5s
      max_retry_wait 300s
      retry_wait 5s
      disable_retry_limit
    </match>
    <match **>
       type s3
       aws_key_id ${AWS_ACCESS_KEY_ID}
       aws_sec_key ${AWS_SECRET_ACCESS_KEY}
       s3_bucket ${AWS_S3_BUCKET}
       s3_region ${AWS_REGION}
       use_ssl
       path logs/
       buffer_path /data/fluentd/s3/
       time_slice_format %Y%m%d%H
       time_slice_wait 10m
       format json
       include_time_key true
       include_tag_key true
       utc
       buffer_chunk_limit 256m
    </match>
    <match **>
       type cloudwatch_logs
       log_group_name ${LOG_GROUP_NAME}
       log_stream_name ${LOG_STREAM_NAME}
       auto_create_stream true
    </match>

can be represented as a series of numbered "properties" like so:

    source.1.type=monitor_agent
    source.1.bind=0.0.0.0
    source.1.port=24220

    source.2.type=tcp
    source.2.port=5170
    source.2.format=json
    source.2.source_host_key=client_host
    source.2.tag=system

    source.3.type=tcp
    source.3.port=5171
    source.3.format=nginx
    source.3.source_host_key=client_host
    source.3.tag=nginx

    match.1.system.type=record_reformer
    match.1.system.renew_record=false
    match.1.system.enable_ruby=false
    match.1.system.remove_keys=__CURSOR,__REALTIME_TIMESTAMP,__MONOTONIC_TIMESTAMP,_BOOT_ID,_UID,_GID,_CAP_EFFECTIVE,_SYSTEMD_SLICE,SYSLOG_IDENTIFIER,_SYSTEMD_CGROUP,_CMDLINE,_COMM
    match.1.system.tag=system.clean

    match.2.**.type=elasticsearch
    match.2.**.log_level=debug
    match.2.**.include_tag_key=true
    match.2.**.hosts=172.17.42.1
    match.2.**.port=9200
    match.2.**.logstash_format=true
    match.2.**.reload_on_failure=true
    match.2.**.reload_connections=true
    match.2.**.buffer_type=file
    match.2.**.buffer_path=/data/fluentd/buffer/
    match.2.**.flush_interval=5s
    match.2.**.max_retry_wait=300s
    match.2.**.retry_wait=5s
    match.2.**.disable_retry_limit

    match.3.**.type=s3
    match.3.**.aws_key_id=${AWS_ACCESS_KEY_ID}
    match.3.**.aws_sec_key=${AWS_SECRET_ACCESS_KEY}
    match.3.**.s3_bucket=${AWS_S3_BUCKET}
    match.3.**.s3_region=${AWS_REGION}
    match.3.**.use_ssl
    match.3.**.path=logs/
    match.3.**.buffer_path=/data/fluentd/s3/
    match.3.**.time_slice_format=%Y%m%d%H
    match.3.**.time_slice_wait=10m
    match.3.**.format=json
    match.3.**.include_time_key=true
    match.3.**.include_tag_key=true
    match.3.**.utc
    match.3.**.buffer_chunk_limit=256m

    match.4.**.type=cloudwatch_logs
    match.4.**.log_group_name=${LOG_GROUP_NAME}
    match.4.**.log_stream_name=${LOG_STREAM_NAME}
    match.4.**.auto_create_stream=true

and then chained together with semicolons like this:

    $ docker run -d \
      -e AWS_REGION='' \
      -e AWS_ACCESS_KEY_ID='' \
      -e AWS_SECURE_ACCESS_KEY='' \
      -e FLUENTD_CONFIG_ONELINE='source.1.type=monitor_agent;source.1.bind=0.0.0.0;source.1.port=24220;source.2.type=tcp;source.2.port=5170;source.2.format=json;source.2.source_host_key=client_host;source.2.tag=system;source.3.type=tcp;source.3.port=5171;source.3.format=nginx;source.3.source_host_key=client_host;source.3.tag=nginx;match.1.system.type=record_reformer;match.1.system.renew_record=false;match.1.system.enable_ruby=false;match.1.system.remove_keys=__CURSOR,__REALTIME_TIMESTAMP,__MONOTONIC_TIMESTAMP,_BOOT_ID,_UID,_GID,_CAP_EFFECTIVE,_SYSTEMD_SLICE,SYSLOG_IDENTIFIER,_SYSTEMD_CGROUP,_CMDLINE,_COMM;match.1.system.tag=system.clean;match.2.**.type=elasticsearch;match.2.**.log_level=debug;match.2.**.include_tag_key=true;match.2.**.hosts=172.17.42.1;match.2.**.port=9200;match.2.**.logstash_format=true;match.2.**.reload_on_failure=true;match.2.**.reload_connections=true;match.2.**.buffer_type=file;match.2.**.buffer_path=/data/fluentd/buffer/;match.2.**.flush_interval=5s;match.2.**.max_retry_wait=300s;match.2.**.retry_wait=5s;match.2.**.disable_retry_limit;match.3.**.type=s3;match.3.**.aws_key_id=${AWS_ACCESS_KEY_ID};match.3.**.aws_sec_key=${AWS_SECRET_ACCESS_KEY};match.3.**.s3_bucket=${AWS_S3_BUCKET};match.3.**.s3_region=${AWS_REGION};match.3.**.use_ssl;match.3.**.path=logs/;match.3.**.buffer_path=/data/fluentd/s3/;match.3.**.time_slice_format=%Y%m%d%H;match.3.**.time_slice_wait=10m;match.3.**.format=json;match.3.**.include_time_key=true;match.3.**.include_tag_key=true;match.3.**.utc;match.3.**.buffer_chunk_limit=256m;match.4.**.type=cloudwatch_logs;match.4.**.log_group_name=${LOG_GROUP_NAME};match.4.**.log_stream_name=${LOG_STREAM_NAME};match.4.**.auto_create_stream=true" \
      -p 5170:5170 \
      ianblenke/fluentd

Ugly? Perhaps. Functional? You bet. 

## Optional, build and run the image from source

If you prefer to build from source rather than use the [ianblenke/fluentd][1] trusted build published to the public Docker Registry, execute the following:

    $ git clone https://github.com/ianblenke/docker-fluentd.git
    $ cd docker-fluentd

If you are using Vagrant, start and provision a virtual machine using the provided Vagrantfile:

    $ vagrant up
    $ vagrant ssh
    $ cd /vagrant

From there, build and run a container using the newly created virtual machine:

    $ make build
    $ make <options> run

You can now verify the fluentd installation by visiting the fluentd monitoring port (24220) via http.

## Contributing

1. Fork it
2. Create your feature branch (`git checkout -b my-new-feature`)
3. Commit your changes (`git commit -am 'Add some feature'`)
4. Push to the branch (`git push origin my-new-feature`)
5. Create new Pull Request

## License

This application is distributed under the [Apache License, Version 2.0][3].

[1]: https://registry.hub.docker.com/u/ianblenke/fluentd
[2]: http://www.fluentd.org/
[3]: http://www.apache.org/licenses/LICENSE-2.0
